x-airflow-common: &airflow-common
  build:
    context: ./docker/docker-airflow
    dockerfile: Dockerfile
  env_file:
    - .env
  user: "${AIRFLOW_UID:-50000}:0"
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    PYTHONPATH: "/opt/airflow/dags:/opt/airflow/plugins"
    # AIRFLOW_CONN_SPARK_DEFAULT: "spark://spark-master:7077"

    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
  volumes:
    - ./dags:/opt/airflow/dags
    - ./plugins:/opt/airflow/plugins
    - ./logs:/opt/airflow/logs
    - ./spark/apps:/opt/airflow/jobs  

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}", "-d", "${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always

  airflow-init:
    <<: *airflow-common
    user: "0:0"
    entrypoint: /bin/bash
    command:
      - -c
      - |
        set -euo pipefail
        echo "[init] wait for DB..."
        until airflow db check >/dev/null 2>&1; do sleep 3; done
        airflow db upgrade || airflow db init
        airflow users create \
          --username "${_AIRFLOW_WWW_USER_USERNAME:-airflow}" \
          --firstname Admin --lastname User \
          --role Admin \
          --email "${_AIRFLOW_WWW_USER_EMAIL:-airflow@example.com}" \
          --password "${_AIRFLOW_WWW_USER_PASSWORD:-airflow}" || true
        airflow pools set riot-api-pool 5 "Riot API rate-limit pool" || true
        echo "[init] done."
    depends_on:
      postgres:
        condition: service_healthy
    restart: "no"

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8081:8080"
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type WebServerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--hostname", "$${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    restart: always

  airflow-dag-processor:
    <<: *airflow-common
    command: dag-processor
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    restart: always

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    restart: always

  ##여기까지 airflow 관련


  spark-master:
    image: bitnami/spark:3.5.1
    env_file:  
      - .env 
    environment:
      - SPARK_MODE=master
      - SPARK_SUBMIT_OPTS=-Duser.home=/opt/bitnami
      - SPARK_EXTRA_OPTS=--conf spark.jars.ivy=/opt/bitnami/.ivy2

    ports:
      - "9090:8080"  
      - "7077:7077"  
    volumes:
      - ./spark/apps:/opt/airflow/jobs
      - ./spark/data:/opt/bitnami/spark/data

    restart: unless-stopped     
    healthcheck:                  
      test: ["CMD", "curl", "-f", "http://localhost:8080"] 
      interval: 30s
      timeout: 5s
      retries: 5

  spark-worker:
    image: bitnami/spark:3.5.1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077

    
    depends_on:
      - spark-master

    volumes:
      - ./spark/apps:/opt/airflow/jobs
      - ./spark/data:/opt/bitnami/spark/data

##여기까지 spark 관련


##여기까지 kafka 관련

volumes:
  postgres-db-volume:






# x-airflow-common: &airflow-common
#   build: 
#     context: ./docker/docker-airflow
#     dockerfile: Dockerfile
#   env_file: 
#    - .env
#   user: "${AIRFLOW_UID}:0"
#   environment:
#     AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
#     AIRFLOW__FAB__SESSION_BACKEND: database
#     AIRFLOW__CORE__EXECUTOR: LocalExecutor
#     AIRFLOW__CORE__LOAD_EXAMPLES: "false"
#     AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
#     AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
#     AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
#     SNOWFLAKE_USER: ${SNOWFLAKE_USER}
#     SNOWFLAKE_PASSWORD: ${SNOWFLAKE_PASSWORD}
#     SNOWFLAKE_ACCOUNT: ${SNOWFLAKE_ACCOUNT}
#     PYTHONPATH: "/opt/airflow/dags:/opt/airflow/plugins"
#     AIRFLOW__SCHEDULER__WORKER_ENABLE_API: "true"
#     AIRFLOW__SCHEDULER__WORKER_API_HOST: "127.0.0.1"
#     AIRFLOW__SCHEDULER__WORKER_API_PORT: "8793"
#     AIRFLOW__SDK__API_BASE_URL: http://airflow-apiserver:8080
#     AIRFLOW__SDK__ENABLE: "true"



#   volumes:
#     - ./dags:/opt/airflow/dags
#     - ./plugins:/opt/airflow/plugins
#     - ./logs:/opt/airflow/logs


# services:
#   postgres:
#     image: postgres:15
#     environment:
#       POSTGRES_USER: ${POSTGRES_USER}
#       POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
#       POSTGRES_DB: ${POSTGRES_DB}
#     volumes:
#       - postgres-db-riot-volume:/var/lib/postgresql/data
#     healthcheck:
#       test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}", "-d", "${POSTGRES_DB}"]

#     restart: always

#   airflow-init:
#     <<: *airflow-common
#     user: "0:0"
#     environment:
     
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
#     depends_on:
#       postgres:
#         condition: service_healthy
#     command: bash -c "airflow db migrate"
#     restart: "no"

#   airflow-apiserver:
#     <<: *airflow-common
#     command: api-server
#     environment:
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
#     ports:
#       - "8081:8080"
#     healthcheck:
#       test: ["CMD-SHELL", 'airflow jobs check --job-type ApiServerJob --hostname "$${HOSTNAME}"']
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always

#     depends_on:
#       airflow-init:
#         condition: service_completed_successfully
      

#   airflow-dag-processor:
#     <<: *airflow-common
#     command: dag-processor
#     environment:
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
#     healthcheck:
#       test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       airflow-init:
#         condition: service_completed_successfully

#   airflow-scheduler:
#     <<: *airflow-common
#     command: scheduler
#     environment:
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
#     depends_on:
#       airflow-init:
#         condition: service_completed_successfully
##여기까지 airflow 관련


  # spark-master:
  #   image: bitnami/spark:3.5
  #   environment:
  #     SPARK_MODE: master
  #     SPARK_SUBMIT_OPTS: -Duser.home=/opt/bitnami
  #     SPARK_EXTRA_OPTS: --conf spark.jars.ivy=/opt/bitnami/.ivy2

  
  #   ports:
  #     - "9090:8080"  
  #     - "7077:7077"  

  #   volumes:
  #     - ./spark/apps:/opt/bitnami/spark/apps
  #     - ./spark/data:/opt/bitnami/spark/data

  # spark-worker:
  #   image: bitnami/spark:3.5
  #   environment:
  #     SPARK_MODE: worker
  #     SPARK_MASTER_URL: spark://spark-master:7077

    
  #   depends_on:
  #     - spark-master

  #   volumes:
  #     - ./spark/apps:/opt/bitnami/spark/apps
  #     - ./spark/data:/opt/bitnami/spark/data

##여기까지 spark 관련


##여기까지 kafka 관련

# volumes:
#   postgres-db-riot-volume:
#   airflow-logs:




